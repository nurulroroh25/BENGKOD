# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13wbJ1d1rq3kmPFtv9EhrUf615YM4Euab
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import numpy as np
# 
# # Load the trained model (tuned Random Forest model)
# try:
#     model = joblib.load('random_forest_model.pkl')
# except FileNotFoundError:
#     st.error("Model file not found. Please make sure 'random_forest_model.pkl' is in the same directory.")
#     st.stop()
# 
# # Load the target encoder to inverse transform predictions
# try:
#     # Assuming you saved the target_encoder in the previous steps.
#     # If not, you might need to recreate it based on the original data or save it earlier.
#     # For simplicity, let's assume the classes are ordered alphabetically and match the encoding.
#     # In a real scenario, you should save and load the actual encoder.
#     # target_encoder = joblib.load('target_encoder.pkl') # Uncomment if you saved the encoder
#     target_classes = ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']
#     # Create a dummy encoder for demonstration if the actual encoder wasn't saved
#     from sklearn.preprocessing import LabelEncoder
#     target_encoder = LabelEncoder()
#     target_encoder.fit(target_classes)
# 
# 
# except FileNotFoundError:
#     st.warning("Target encoder file not found. Using default class labels.")
#     target_encoder = None
#     target_classes = ['0', '1', '2', '3', '4', '5', '6'] # Fallback to encoded labels
# 
# # Streamlit UI
# st.set_page_config(page_title="Prediksi Tingkat Obesitas", layout="wide")
# 
# st.title('Prediksi Tingkat Obesitas')
# 
# st.write("""
# Selamat datang di aplikasi prediksi tingkat obesitas!
# Aplikasi ini menggunakan model machine learning (Random Forest) yang telah dilatih
# menggunakan fitur-fitur yang paling berpengaruh untuk memprediksi kategori obesitas
# berdasarkan input Anda.
# 
# Silakan masukkan informasi berikut:
# """)
# 
# # Input features from user - ONLY the influential features
# st.header("Masukkan Data Diri dan Kebiasaan")
# 
# col1, col2 = st.columns(2)
# 
# with col1:
#     weight = st.number_input('Berat (kg)', min_value=10.0, max_value=250.0, value=70.0, help="Masukkan berat badan Anda dalam kilogram.")
#     faf = st.slider('Frekuensi Aktivitas Fisik (skala 0-3)', 0.0, 3.0, 1.0, help="Skala 0 (tidak pernah) hingga 3 (setiap hari).")
#     fcvc = st.slider('Konsumsi Sayur dan Buah (skala 1-3)', 1.0, 3.0, 2.0, help="Skala 1 (jarang) hingga 3 (sering).")
# 
# 
# with col2:
#     family_history_with_overweight = st.selectbox('Riwayat Keluarga Obesitas?', ['no', 'yes'], help="Apakah ada riwayat obesitas dalam keluarga Anda?")
#     mtrans = st.selectbox('Transportasi Utama', ['Public_Transportation', 'Walking', 'Automobile', 'Motorbike', 'Bike'], help="Pilih moda transportasi yang paling sering Anda gunakan.")
# 
# 
# # Map categorical inputs to numerical values based on the encoding used during training
# # This mapping should be consistent with the LabelEncoding done in the notebook
# family_history_with_overweight_map = {'no': 0, 'yes': 1} # Assuming '?' was encoded as 2 and is handled by imputation
# mtrans_map = {'Public_Transportation': 3, 'Walking': 4, 'Automobile': 0, 'Motorbike': 2, 'Bike': 1} # Assuming '?' was encoded as 5 and is handled by imputation
# 
# # Create a DataFrame with the input data - ONLY with selected features
# input_data = pd.DataFrame({
#     'Weight': [weight],
#     'family_history_with_overweight': [family_history_with_overweight_map[family_history_with_overweight]],
#     'FAF': [faf],
#     'MTRANS': [mtrans_map[mtrans]],
#     'FCVC': [fcvc]
# })
# 
# # IMPORTANT: Apply the same scaling used during training for the selected features
# # If you used MinMaxScaler, you need to load/recreate the scaler and apply it to the input_data
# # Since we trained on scaled selected features, we need to scale the input data here.
# # We need to fit the scaler on X_train_selected and then transform the input_data.
# 
# # For this to work correctly, you would ideally save the scaler object during training
# # and load it here. As a workaround for this example, we will fit a new scaler on the
# # selected training data (assuming X_train_selected is available in this context,
# # which it won't be in a real deployment).
# # A better approach in a real app is to save the scaler after fitting on X_train_selected
# # and load it here.
# 
# # Dummy scaler fitting (replace with loading a saved scaler in production)
# from sklearn.preprocessing import MinMaxScaler
# # For demonstration purposes, let's manually scale based on the min/max of the selected features from df_selected
# # This is a simplified approach and assumes df_selected min/max are representative
# # In a real app, use the scaler fitted on X_train_selected
# 
# # Calculate min/max from the df_selected used for training
# # This requires df_selected to be available, which is not ideal for deployment
# # Assume df_selected is available for demonstration purposes.
# # min_vals = df_selected.drop('NObeyesdad', axis=1).min()
# # max_vals = df_selected.drop('NObeyesdad', axis=1).max()
# 
# # A more robust way for deployment is to save the min/max values during training
# # and load them here. Let's use hardcoded values based on the previous `df_selected.describe()` output
# # from the notebook for the selected features:
# # 'Weight': min=39.0, max=250.0 (after outlier removal) -> scaled min=0, max=1
# # 'family_history_with_overweight': min=0.0, max=1.0 -> scaled min=0, max=1
# # 'FAF': min=0.0, max=3.0 -> scaled min=0, max=1
# # 'MTRANS': min=0.0, max=4.0 -> scaled min=0, max=1
# # 'FCVC': min=1.0, max=3.0 -> scaled min=0, max=1
# 
# 
# min_vals_hardcoded = pd.Series({
#     'Weight': 39.0,
#     'family_history_with_overweight': 0.0,
#     'FAF': 0.0,
#     'MTRANS': 0.0,
#     'FCVC': 1.0
# })
# 
# max_vals_hardcoded = pd.Series({
#     'Weight': 250.0,
#     'family_history_with_overweight': 1.0,
#     'FAF': 3.0,
#     'MTRANS': 4.0,
#     'FCVC': 3.0
# })
# 
# 
# # Apply manual scaling using hardcoded min/max
# input_data_scaled = (input_data - min_vals_hardcoded) / (max_vals_hardcoded - min_vals_hardcoded)
# 
# 
# # Make prediction
# if st.button('Prediksi Tingkat Obesitas'):
#     # Ensure column order is the same as training data
#     input_data_scaled = input_data_scaled[['Weight', 'family_history_with_overweight', 'FAF', 'MTRANS', 'FCVC']]
# 
#     prediction_encoded = model.predict(input_data_scaled)
# 
#     # Inverse transform the prediction to get the original class label
#     if target_encoder:
#         prediction_label = target_encoder.inverse_transform(prediction_encoded)
#         st.success(f'Hasil Prediksi: **{prediction_label[0]}**')
#     else:
#         st.success(f'Hasil Prediksi (Encoded): **{prediction_encoded[0]}**')
# 
# st.markdown("---")
# st.write("Model dilatih menggunakan data obesitas dan fitur-fitur yang paling berpengaruh.")